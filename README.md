RAG Document Q&A System


This project implements a Retrieval-Augmented Generation (RAG) system that allows users to upload documents (PDF, TXT, DOCX), process their content, and ask questions to get answers grounded in the document text. The system leverages various open-source libraries and models to build the RAG pipeline and provide a user-friendly interface.


Chosen Tech Stack:

Document Loading: Langchain Document Loaders (PyPDFLoader, TextLoader, Docx2txtLoader)
Text Chunking: Custom Python function
Embedding Model: Sentence-Transformers (sentence-transformers/multi-qa-mpnet-base-dot-v1)
Vector Database: FAISS (via faiss-cpu)
Retrieval & Generation Orchestration: Langchain Runnables and Prompt Templates
Language Model (LLM): Ollama (running locally within the environment, using the llama2 model by default)
Ollama Interaction: ollama Python library and Langchain's ChatOllama
User Interface: Gradio
Environment: Google Colab (or a compatible Python environment)


Setup Instructions

To get this RAG system up and running, follow these steps:
Environment: The code is developed and tested in Google Colab. You can run it directly in a Colab environment. Ensure you have access to a runtime with sufficient resources (CPU and RAM; a GPU is recommended for faster Ollama inference).
Run Ollama Locally (Recommended for performance) or in Colab:

Option 1: Run Ollama Locally and Expose with ngrok (Recommended):
Download and install Ollama from ollama.com/download on your local machine.
Pull the desired model (e.g., llama2) locally by running ollama pull llama2 in your terminal.
Download and install ngrok from ngrok.com.
Start ngrok to expose your local Ollama server (default port 11434): ngrok http 11434.
Copy the generated https Forwarding URL from the ngrok output.
In the Colab notebook, update the host parameter in the ollama.Client initialization (cell 25286126) to this ngrok URL: client = ollama.Client(host='YOUR_NGROK_URL').

Option 2: Run Ollama Directly in Colab (Easier setup, potentially slower):
Execute the Colab cells provided for installing and starting the Ollama server within the Colab runtime and pulling the necessary model (e.g., llama2). These cells are typically labeled "Install Ollama", "Start Ollama Server", "Download Ollama Model", and "Verify Ollama Status".
Ensure the ollama.Client initialization in the notebook (cell 25286126) is set to the default host='http://localhost:11434'.
Open the Colab Notebook: Open the provided .ipynb notebook file in Google Colab.
Execute Cells: Run all the code cells in the notebook sequentially from top to bottom. This will install dependencies, set up Ollama (if running in Colab), define functions, and launch the Gradio interface.
Use the Gradio Interface: Once the last cell (containing the Gradio interface code) has finished executing, the Gradio web interface will appear in the output area of that cell.


Process Document:

In the Gradio interface, click "Upload Document" and select your PDF, TXT, or DOCX file.
Click the "Process Document" button. Wait for the status to show "Document processed and ready!". Check the Colab output for any errors if it fails.
Ask Questions: Once processing is complete, type your question about the document content into the "Ask a question about the Document" textbox.
Press Enter or click the submit button. The generated answer will appear in the "Answer" textbox.


Features
This RAG system provides the following features:
Multi-format Document Support: Load and process documents in various formats, including PDF, TXT, and DOCX.
Grounded Q&A: Ask questions about the content of your uploaded documents and receive answers generated by an LLM that are grounded in the document text.
Flexible LLM: Utilizes Ollama, allowing you to easily switch between different models available on your Ollama server (e.g., llama2, Mistral, etc.).
Interactive Web UI: A user-friendly interface built with Gradio for easy document upload, processing, and question answering directly within a web browser (or Colab output).
Modular RAG Pipeline: Built using key components like Langchain for loading and orchestration, Sentence-Transformers for embeddings, and FAISS for vector storage.




How It Works
The application follows a standard Retrieval-Augmented Generation (RAG) pipeline. The user interacts with this pipeline through a Gradio web interface:

Ingestion: The document loader (using Langchain's PyPDFLoader, TextLoader, Docx2txtLoader) fetches content from your uploaded PDF, TXT, or DOCX file.
Chunking: A custom chunking function splits the document content into smaller, overlapping text chunks.
Indexing: The system uses the sentence-transformers/multi-qa-mpnet-base-dot-v1 model to create a numerical vector (embedding) for each chunk and stores them in a FAISS vector database (VectorStore).
Retrieval & Generation: When a question is asked, the system retrieves the most relevant chunks from the FAISS vector store based on the question's embedding. The top-ranked chunks and the original question are then passed to the Ollama LLM within a structured prompt (using Langchain Prompt Templates and Runnables), which generates the final, grounded answer.

User Interaction: The Gradio web interface provides a user-friendly way to upload documents, trigger the processing steps, ask questions, and view the generated answers directly in a web browser or Colab output.
