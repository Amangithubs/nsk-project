{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1kWZQX-mWIupzRS_IMcTC6B63sSBGRjV0",
      "authorship_tag": "ABX9TyNXKtJjpWb/Iz8cxm33yOcv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amangithubs/nsk-project/blob/main/NSK_AI_final_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bdb5e96"
      },
      "source": [
        "## Document Ingestion\n",
        "\n",
        "This section covers loading document content and preparing it for processing, including installing necessary libraries and defining the function to load different file types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63cae7ce"
      },
      "source": [
        "!pip install PyPDF2 --quiet # PyPDF2 is needed by PyPDFLoader\n",
        "!pip install langchain --quiet\n",
        "!pip install langchain-community --quiet\n",
        "!pip install pypdf --quiet # Install pypdf explicitly as required by PyPDFLoader\n",
        "!pip install docx2txt --quiet # Required for Docx2txtLoader\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
        "import os\n",
        "\n",
        "def load_document(file_path):\n",
        "    \"\"\"\n",
        "    Loads content from a document based on its file type using Langchain loaders.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the document file.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of page content strings extracted from the document.\n",
        "        Returns an empty list if the file type is not supported or loading fails.\n",
        "    \"\"\"\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    file_extension = file_extension.lower()\n",
        "\n",
        "    documents = []\n",
        "    try:\n",
        "        if file_extension == \".pdf\":\n",
        "            loader = PyPDFLoader(file_path)\n",
        "            documents = loader.load()\n",
        "        elif file_extension == \".txt\":\n",
        "            loader = TextLoader(file_path)\n",
        "            documents = loader.load()\n",
        "        elif file_extension == \".docx\":\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "            documents = loader.load()\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {file_extension}\")\n",
        "            return [] # Return empty list for unsupported types\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading document {file_path}: {e}\")\n",
        "        return [] # Return empty list on error\n",
        "\n",
        "    # Extract text content from Langchain Document objects\n",
        "    pages_text = [doc.page_content for doc in documents]\n",
        "    return pages_text\n",
        "\n",
        "# Note: The process_document function (in cell iw7IqxGzZzYg9) will need to be updated\n",
        "# to call this new load_document function instead of load_pdf_pages,\n",
        "# and to pass the file path correctly."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0573aed1"
      },
      "source": [
        "## Language Model (Ollama) Setup\n",
        "\n",
        "This section covers downloading, installing, and running the Ollama server and downloading the necessary language models (LLMs) that the RAG pipeline will use for text generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "278cd2cd"
      },
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91e86dd"
      },
      "source": [
        "# Start the Ollama server in the background\n",
        "# This might require specific handling to keep it running in Colab\n",
        "# A common approach is to run it as a background process\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"Starting Ollama server...\")\n",
        "process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(5) # Give the server a few seconds to start\n",
        "print(\"Ollama server process started.\")\n",
        "# Note: The server will run in the background as long as this Colab runtime is active."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "e62beeb9"
      },
      "source": [
        "# Download an Ollama model (e.g., llama2)\n",
        "# You can replace 'llama2' with another model name if you prefer\n",
        "print(\"Downloading Ollama model 'llama2'...\")\n",
        "# Use subprocess.run to wait for the download to complete\n",
        "result = subprocess.run([\"ollama\", \"pull\", \"llama2\"], capture_output=True, text=True)\n",
        "print(\"Ollama pull output:\")\n",
        "print(result.stdout)\n",
        "print(result.stderr)\n",
        "\n",
        "if result.returncode != 0:\n",
        "    print(\"Error downloading model. Please check the model name and try again.\")\n",
        "else:\n",
        "    print(\"Model 'llama2' downloaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "25286156"
      },
      "source": [
        "!pip install ollama --quiet\n",
        "import ollama\n",
        "\n",
        "# Configure the Ollama client\n",
        "# Replace with the actual URL of your Ollama server if it's not running on the default host/port\n",
        "# If running locally and need Colab access, you might need to use ngrok or similar.\n",
        "client = ollama.Client(host='http://localhost:11434')\n",
        "\n",
        "# This function is now used by the answer_question function.\n",
        "def generate_answer_with_ollama(prompt, model=\"llama2\"):\n",
        "    \"\"\"\n",
        "    Generates a text completion using an Ollama model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt for text generation.\n",
        "        model (str): The name of the Ollama model to use (e.g., \"llama2\", \"mistral\").\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.generate(\n",
        "            model=model,\n",
        "            prompt=prompt,\n",
        "            stream=False # Set to True if you want streaming responses\n",
        "        )\n",
        "        return response['response']\n",
        "    except ollama.ResponseError as e:\n",
        "        print(f\"Ollama API error: {e}\")\n",
        "        return f\"Error generating answer with Ollama: {e}\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Ollama generation: {e}\")\n",
        "        return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "# Note: The answer_question function (in cell k4A6TI6mzSG2) calls this function."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bef50612"
      },
      "source": [
        "# Verify that Ollama is running and the model is available\n",
        "print(\"Checking Ollama status and model list...\")\n",
        "result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
        "print(\"Ollama list output:\")\n",
        "print(result.stdout)\n",
        "print(result.stderr)\n",
        "\n",
        "if \"llama2\" in result.stdout:\n",
        "    print(\"Ollama is running and 'llama2' model is available.\")\n",
        "else:\n",
        "    print(\"Ollama or 'llama2' model not found. Check previous steps.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "402c6623"
      },
      "source": [
        "## Indexing and Storage\n",
        "\n",
        "This section covers generating numerical representations (embeddings) of the document content and storing them in a searchable vector database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cb3b8fa"
      },
      "source": [
        "!pip install faiss-cpu --quiet\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "class VectorStore:\n",
        "    def __init__(self):\n",
        "        # Updated dimension to match the embedding model (sentence-transformers/multi-qa-mpnet-base-dot-v1)\n",
        "        self.index = faiss.IndexFlatL2(768)\n",
        "        self.texts = []\n",
        "\n",
        "    def add(self, text, embedding):\n",
        "        self.index.add(np.array([embedding]).astype('float32'))\n",
        "        self.texts.append(text)\n",
        "\n",
        "    def search(self, query_embedding, top_k=3):\n",
        "        D, I = self.index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
        "        return [self.texts[i] for i in I[0]]\n",
        "\n",
        "# Note: This class is used by process_document and answer_question."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rpOLYhSCzY7M"
      },
      "source": [
        "!pip install sentence-transformers --quiet\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the sentence transformer model\n",
        "# Using the same model as before for consistency with FAISS dimension\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
        "\n",
        "def get_embedding(text):\n",
        "    # Use the sentence_transformers library to get embeddings\n",
        "    embedding = embedding_model.encode(text)\n",
        "    return embedding.tolist() # Return as a list to match previous format if needed\n",
        "\n",
        "# Note: This function is called by process_document and answer_question."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw7IqxGzZzYg9"
      },
      "source": [
        "store = VectorStore()\n",
        "\n",
        "def process_document(file_input): # Renamed and changed parameter name for clarity\n",
        "    print(\"--- Starting process_document function ---\") # Debug print\n",
        "\n",
        "    # Gradio File component provides a path via file_input.name\n",
        "    file_path = file_input.name if file_input else None\n",
        "\n",
        "    if not file_path:\n",
        "        print(\"Error: No file uploaded.\")\n",
        "        return \"Error: No file uploaded.\"\n",
        "\n",
        "    pages = [] # Initialize pages list\n",
        "    try:\n",
        "        print(f\"--- Calling load_document for {file_path} ---\") # Debug print\n",
        "        pages = load_document(file_path) # Call the new function\n",
        "        print(f\"--- load_document returned {len(pages) if pages else 0} pages ---\") # Debug print\n",
        "\n",
        "        # Debugging print: Show extracted text or indicate if extraction failed\n",
        "        if not pages:\n",
        "            print(\"Error: No text extracted from the document by load_document.\")\n",
        "            return \"Error: No text extracted from the document by load_document.\"\n",
        "        else:\n",
        "            print(f\"Successfully extracted text from {len(pages)} pages.\")\n",
        "            # Optionally print a snippet of the extracted text\n",
        "            # print(\"Extracted text snippet (first 500 chars):\")\n",
        "            # print(\"\".join(pages)[:500])\n",
        "            # print(\"-\" * 20)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during load_document for {file_path}: {e}\")\n",
        "        print(\"-\" * 20)\n",
        "        return f\"Error during document loading: {e}\"\n",
        "\n",
        "\n",
        "    # Clear the store before adding new data from a new document\n",
        "    print(\"--- Clearing/Initializing VectorStore ---\") # Debug print\n",
        "    global store\n",
        "    store = VectorStore()\n",
        "    print(\"--- VectorStore cleared/initialized ---\") # Debug print\n",
        "\n",
        "\n",
        "    print(\"--- Starting chunking and embedding loop ---\") # Debug print\n",
        "    for i, text in enumerate(pages):\n",
        "        print(f\"--- Processing page {i+1}/{len(pages)} ---\") # Debug print\n",
        "        chunks = chunk_text(text)\n",
        "        print(f\"--- Page {i+1} chunked into {len(chunks)} chunks ---\") # Debug print\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            # print(f\"--- Embedding chunk {j+1}/{len(chunks)} of page {i+1} ---\") # Too verbose?\n",
        "            try:\n",
        "                emb = get_embedding(chunk)\n",
        "                # print(f\"--- Embedding obtained for chunk {j+1} ---\") # Too verbose?\n",
        "                # print(f\"--- Adding chunk {j+1} to VectorStore ---\") # Too verbose?\n",
        "                store.add(chunk, emb)\n",
        "                # print(f\"--- Chunk {j+1} added to VectorStore ---\") # Too verbose?\n",
        "            except Exception as e:\n",
        "                print(f\"Error embedding or adding chunk: {e}\")\n",
        "                # Continue processing other chunks even if one fails\n",
        "                pass\n",
        "\n",
        "    print(\"--- Chunking and embedding loop finished ---\") # Debug print\n",
        "    return \"Document processed and ready!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "605f058c"
      },
      "source": [
        "## RAG Pipeline Core (Retrieval & Generation)\n",
        "\n",
        "This section covers both the **retrieval** of relevant document chunks from the vector store (using the VectorStore's search method) and the **generation** of the final answer. The generation uses the retrieved context and the user query to create a grounded answer using a language model (Ollama), implemented with Langchain Runnables and Prompt Templates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4A6TI6mzSG2"
      },
      "source": [
        "def chunk_text(text, chunk_size=1000, overlap=200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "class VectorStore:\n",
        "    def __init__(self):\n",
        "        # Updated dimension to match the embedding model (sentence-transformers/multi-qa-mpnet-base-dot-v1)\n",
        "        self.index = faiss.IndexFlatL2(768)  # embedding dim assumed 768, adjust if needed\n",
        "        self.texts = []\n",
        "\n",
        "    def add(self, text, embedding):\n",
        "        self.index.add(np.array([embedding]).astype('float32'))\n",
        "        self.texts.append(text)\n",
        "\n",
        "    def search(self, query_embedding, top_k=5):\n",
        "        D, I = self.index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
        "        return [self.texts[i] for i in I[0] if i != -1]\n",
        "\n",
        "# Note: The get_embedding function is defined in cell rpOLYhSCzY7M\n",
        "# The ollama client is initialized in cell 25286156\n",
        "# The generate_answer_with_ollama function is defined in cell 25286156\n",
        "\n",
        "# Import Langchain components for generation\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# Import ChatOllama from the new package\n",
        "!pip install -U langchain-ollama --quiet # Already installed in previous step\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "\n",
        "# Define the Langchain components for the RAG chain\n",
        "# Assuming Ollama server is running at localhost:11434 and model 'llama2' is pulled\n",
        "# You might need to adjust model name and base_url if different\n",
        "llm = ChatOllama(model=\"llama2\", base_url=\"http://localhost:11434\")\n",
        "\n",
        "# Define the prompt template using Langchain\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\")\n",
        "\n",
        "# Create a RAG chain using Langchain Runnables\n",
        "# This chain takes context and question, formats the prompt, and passes it to the LLM\n",
        "rag_chain = (\n",
        "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "def answer_question(question):\n",
        "    print(\"--- Starting answer_question function (Langchain Runnables) ---\")\n",
        "    try:\n",
        "        print(\"--- Getting question embedding ---\")\n",
        "        q_emb = get_embedding(question)\n",
        "        print(\"--- Question embedding obtained ---\")\n",
        "\n",
        "        print(\"--- Searching vector store ---\")\n",
        "        # Increase top_k to retrieve more context\n",
        "        context_docs = store.search(q_emb, top_k=5)\n",
        "        # Join the retrieved text chunks\n",
        "        context_text = \"\\n\\n\".join(context_docs)\n",
        "        print(\"--- Vector store search completed ---\")\n",
        "\n",
        "        # Debugging prints\n",
        "        print(\"Retrieved Context:\")\n",
        "        print(context_text)\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        prompt = context_text + f\"\\n\\nQuestion: {question}\\nAnswer:\" # Prompt created by Langchain template now\n",
        "\n",
        "        print(\"Generated Prompt:\")\n",
        "        print(prompt)\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        print(\"--- Invoking Langchain RAG chain ---\")\n",
        "        # Invoke the Langchain RAG chain with context and question\n",
        "        response = rag_chain.invoke({\"context\": context_text, \"question\": question})\n",
        "        print(\"--- Langchain RAG chain invocation completed ---\")\n",
        "\n",
        "        # Debugging print for successful response\n",
        "        print(\"Model Response:\")\n",
        "        print(response)\n",
        "        print(\"-\" * 20)\n",
        "        return response\n",
        "\n",
        "    except Exception as outer_e:\n",
        "        # Catch any other errors in the function\n",
        "        print(f\"An unexpected error occurred in answer_question: {outer_e}\")\n",
        "        print(\"-\" * 20)\n",
        "        return f\"An unexpected error occurred: {outer_e}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40f51042"
      },
      "source": [
        "## User Interaction\n",
        "\n",
        "This section covers setting up the user interface using Gradio to allow users to upload documents, process them, ask questions, and view answers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b447b08f"
      },
      "source": [
        "!pip install gradio --quiet\n",
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        pdf_input = gr.File(label=\"Upload Document\") # Changed label\n",
        "        upload_btn = gr.Button(\"Process Document\") # Changed label\n",
        "        status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "    # Call the new process_document function\n",
        "    upload_btn.click(process_document, inputs=[pdf_input], outputs=[status])\n",
        "\n",
        "    question = gr.Textbox(label=\"Ask a question about the Document\") # Changed label\n",
        "    answer = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "    question.submit(answer_question, inputs=[question], outputs=[answer])\n",
        "\n",
        "demo.launch()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}